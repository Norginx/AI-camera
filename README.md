# 1.项目简介
本项目是基于旭日派X3的AI导盲眼镜。主要功能为：利用摄像头抓取环境照片，将图片数据传给llava模型，并填入相对应的提示词，由llava模型将图片的识别结果返回。并通过文字转tts工具实现播报。

# 2.实现原理
嵌入式平台采集照片转换为base64通过api提交到局域网内的大模型接口，大模型处理完后将处理的结果以中文字符串的形式返回给嵌入式平台，嵌入式平台再通过将字符串提交到一个在线的tts平台，tts平台会返回一个.mp3格式的文件，播放这个文件便可实现语音播报。
## 2.1大模型的部署
llava大模型部署在了我的PC设备上，我的PC设备上,搭载了2070super显卡和R5 5600的CPU。

大模型容器使用的是Ollama，是在Windows环境下部署的。在此处为了便于PC的大模型和我的嵌入式平台实现交互，我们要在Windows的环境变量中。
    
    环境变量名：OLLAMA_HOST
    变量值：0.0.0.0:11434
修改完成后，在同一个局域网的环境内，通过访问部署ollama的电脑ip下的11434端口，会显示 ollama is running 这就表面我们的ollama部署已经完成了。

## 2.2旭日派X3的配置
此部分比较简单，和树莓派类似，唯一比较鸡肋的是旭日派X3基础的板子并没有音频接口，所以我们需要使用蓝牙去连接我们的音频设备，以下是一些在控制台的操作。

    bluetoothctl scan on    //开启蓝牙扫描

    /*扫描出来的结果一般为 
    [NEW] Device 12:11:72:68:18:CE 联想 thinkplus-TS33
    这种形式*/

    bluetoothctl discoverable on //把本机设为可被扫描到的状态

    bluetoothctl pair 12:11:72:68:18:CE  //配对设备

    bluetoothctl paired-devices //查看已经配对的设备

    bluetoothctl connect 12:11:72:68:18:CE //连接蓝牙设备

    bluetoothctl trust 12:11:72:68:18:CE //信任该设备，自动连接

配置完成以后，把代码仓库里的main.py拷贝到嵌入式平台上。运行即可

## 2.3Python代码
该部分主要用到了一些库，大家运行前要提前安装好。还用到了一个X3派中特有的GPIO库，该库用于接入一个按键实现按下按键触发的功能，如果要移植到其他地方去这个地方要做相对应的修改。代码的实现，大家可以去阅读一下我的源码，还是比较简单的。

# 3总结
该项目的灵感来源于一个B站UP主，该UP主的代码是部署在树莓派上的，而且是完全离线本地化运行。这里就会涉及到了几个问题：
    
    1.ollama大模型部署至少需要8g的运存，本人的开发板只有4g（树莓派5b 8g版的太贵了，买不起）
    2.大模型运行速度非常慢，据up所说处理一张图片要用5分钟。
    3.由于是本地化，所以用的文字转语音的引擎也是Ubuntu自带 的，声音非常的诡异。
所以在这里我稍微做了下修改，把它改成了用嵌入式平台访问API的方式，把模型的运算放到了算力更强的PC平台。综合下来，采集一张图片到处理完的用时在1分钟以内（取决于网络环境）。大大提高了运行效率。

最后，非常感谢那位UP主，这里挂上他的视频链接：
https://www.bilibili.com/video/BV1wm42137ut/?spm_id_from=333.337.search-card.all.click


